{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open libraries\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# For EDA\n",
    "from collections import Counter\n",
    "# For active learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "import copy # For data processing\n",
    "import os\n",
    "# get the current working directory\n",
    "current_working_directory = os.getcwd()\n",
    "# print output to the console\n",
    "print(current_working_directory)\n",
    "import subprocess\n",
    "import sys\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"rdata\"])\n",
    "import rdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load KewMNIST data\n",
    "kew_mnist = rdata.read_rda('Kew-MNIST-full-dataset.Rdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out objects out of rdata file\n",
    "kew_train_images = kew_mnist['kew_train_images']\n",
    "kew_train_labels = kew_mnist['kew_train_labels']\n",
    "kew_test_images = kew_mnist['kew_test_images']\n",
    "kew_test_labels = kew_mnist['kew_test_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Flower', 'Fruit', 'Leaf', 'Plant-Tag', 'Stem', 'Whole-Plant']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape of samples\n",
    "kew_test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kew_train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train and test subsets\n",
    "merged_images = np.concatenate((kew_train_images, kew_test_images), axis=0) # merge images\n",
    "labels = list(kew_train_labels) + list(kew_test_labels) # merge labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "# Plot pie chart\n",
    "plt.pie(label_counts.values(), labels=[f'{class_names[label]}: {count}' for label, count in label_counts.items()], \n",
    "        colors=plt.cm.Paired.colors)\n",
    "# Title\n",
    "plt.title('KewMNIST label distribution')\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the merged dataset\n",
    "print(merged_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if images and labels match -- use first 25 images\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(merged_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[labels[i]])\n",
    "plt.show()\n",
    "#subplot figures\n",
    "\n",
    "# Print the first 25 labels\n",
    "print(labels[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the basic computer vision model - 23 mins\n",
    "def Basic_CVModel():\n",
    "    tf.random.set_seed(42)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(500, 500)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(6, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Small_CVModel():\n",
    "    # cv small - 13 mins\n",
    "    tf.random.set_seed(42)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(500, 500)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(6, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Big_CVModel():\n",
    "    # Big neural network\n",
    "    tf.random.set_seed(42) # 45 mins\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(500, 500)),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(6, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'])\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_CVModel():\n",
    "    # for cnns 2hrs\n",
    "    # define cnn model\n",
    "    tf.random.set_seed(42)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(500, 500, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(6, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model with RMSprop optimizer\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_AL(model): \n",
    "    merged_images_np = np.array(merged_images)  # Ensure images are NumPy arrays\n",
    "    labels_np = np.array(labels)  # Ensure labels are NumPy arrays\n",
    "\n",
    "    model_metrics = {\n",
    "        'model_accuracy' : [],\n",
    "        'model_loss' : [],\n",
    "        'train_accuracy' : [],\n",
    "        'train_loss' : [],\n",
    "        'mcc' : [],\n",
    "        'f1' : []\n",
    "    }\n",
    "\n",
    "    # make the train test split \n",
    "    train_img, test_img, train_labels, test_labels = train_test_split(\n",
    "        merged_images_np, labels_np, train_size=120, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Calucates split fraction for labeled datset\n",
    "    def calc_fractions(train_labels, sum_labels):\n",
    "        return (len(train_labels)/len(sum_labels))\n",
    "\n",
    "    # calculate fraction of data used for training\n",
    "    training_data_fraction = calc_fractions(train_labels, labels) \n",
    "\n",
    "    print(f\"train_img shape: {train_img.shape}, train_labels shape: {train_labels.shape}\")\n",
    "\n",
    "    iteration = 0\n",
    "\n",
    "    # Start iteration \n",
    "    while training_data_fraction < 0.9: # continue to cycle until training is no more than 90% of total sample size \n",
    "\n",
    "        print (f\"\\n Iteration {iteration} - Training with {len(train_img)} samples\")\n",
    "\n",
    "        # train classifier for this iteration\n",
    "        model_history = model.fit(train_img, train_labels, epochs=10, verbose=2) # fit the model\n",
    "        predictions = model.predict(test_img) # get predictions\n",
    "        predicted_labels = np.argmax(predictions, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "        # Get loss & accuracy for training & test set\n",
    "        train_acc = model_history.history['accuracy'][-1]   # Last epoch accuracy\n",
    "        train_loss = model_history.history['loss'][-1]      # Last epoch loss\n",
    "        test_loss, test_acc = model.evaluate(test_img,  test_labels, verbose=2)\n",
    "        print('\\nTest accuracy:', test_acc) \n",
    "\n",
    "        # get mcc and f1 scores\n",
    "        model_metrics['mcc'].append(matthews_corrcoef(test_labels, predicted_labels))\n",
    "        model_metrics['f1'].append(f1_score(test_labels, predicted_labels, average='weighted') )\n",
    "        print('\\nMCC:', matthews_corrcoef(test_labels, predicted_labels)) \n",
    "        \n",
    "        # store metrics\n",
    "        model_metrics['model_accuracy'].append(test_acc)\n",
    "        model_metrics['model_loss'].append(test_loss)\n",
    "        model_metrics['train_accuracy'].append(train_acc)\n",
    "        model_metrics['train_loss'].append(train_loss)\n",
    "        \n",
    "        # Compute entropy-based uncertainty\n",
    "        uncertainty = entropy(predictions.T)  # Compute entropy for each sample\n",
    "        q_indexes = np.argsort(uncertainty)[-100:]  # Select 100 most uncertain samples\n",
    "\n",
    "        # Add most uncertain cample to training set\n",
    "        train_img = np.vstack((train_img, test_img[q_indexes])) # Place most uncertain samples underneath labeled data\n",
    "        train_labels = np.concatenate((train_labels, test_labels[q_indexes])) # Place labels of most uncertain samples in labelled data\n",
    "        \n",
    "        # Update fraction\n",
    "        labeled_data_fraction = calc_fractions(train_labels, labels) \n",
    "        if labeled_data_fraction > 0.9: # break loop if fraction > 90%\n",
    "            break\n",
    "\n",
    "        # remove the queried samples from the test set\n",
    "        test_img = np.delete(test_img, q_indexes, axis = 0)\n",
    "        test_labels = np.delete(test_labels, q_indexes, axis = 0)\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    # Print final model performance\n",
    "    print(\"\\nFinal Model Performance:\")\n",
    "    print(f\"Train Accuracy: {model_metrics['train_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Test Accuracy: {model_metrics['model_accuracy'][-1]:.4f}\") \n",
    "    return (model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_Architecture = Basic_CVModel()\n",
    "baseNetwork_metrics = CV_AL(Base_Architecture) ## run once - takes 24 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Small_Architecture = Small_CVModel()\n",
    "smallNetwork_metrics = CV_AL(Small_Architecture) ## run once - takes 15 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Big_Architecture = Big_CVModel()\n",
    "BigNetwork_metrics = CV_AL(Big_Architecture) ## run once - takes 45 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_Architecture = CNN_CVModel()\n",
    "CNN_metrics = CV_AL(CNN_Architecture) ## run once - takes 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn results from all cv models into pandas\n",
    "baseNetwork_metrics_df = pd.DataFrame(baseNetwork_metrics)\n",
    "smallNetwork_metrics_df = pd.DataFrame(smallNetwork_metrics)\n",
    "denseNetwork_metrics_df = pd.DataFrame(BigNetwork_metrics)\n",
    "cnn_metrics_df = pd.DataFrame(CNN_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_loss_plots (dataFrame, name):\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(dataFrame['train_accuracy'], label=\"Train Accuracy\")\n",
    "    plt.plot(dataFrame['model_accuracy'], label=\"Test Accuracy\")\n",
    "    plt.xlabel(\"Iteration\", fontsize=10)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=10)\n",
    "    plt.title(\"Model Accuracy\", fontsize=10)\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(dataFrame['train_loss'], label=\"Train Loss\")\n",
    "    plt.plot(dataFrame['model_loss'], label=\"Test Loss\")\n",
    "    plt.xlabel(\"Iteration\", fontsize=10)\n",
    "    plt.ylabel(\"Loss\", fontsize=10, labelpad=2)\n",
    "    plt.title(\"Model Loss\", fontsize=10)\n",
    "\n",
    "    plt.suptitle(name + ' performance')\n",
    "    fig = plt.gcf()\n",
    "    fig.legend(['Train', 'Test'], loc=\"upper left\",  bbox_to_anchor=(0.03, 1), fontsize=10)\n",
    "    plt.subplots_adjust(wspace=0.25)\n",
    "    plt.show()\n",
    "\n",
    "accuracy_loss_plots(baseNetwork_metrics_df, 'Basic Neural Network')\n",
    "accuracy_loss_plots(smallNetwork_metrics_df, 'Small Neural Network')\n",
    "accuracy_loss_plots(denseNetwork_metrics_df, 'Big Neural Network')\n",
    "accuracy_loss_plots(cnn_metrics_df, 'Convoluted Neural Network')\n",
    "\n",
    "\n",
    "# mcc subplots\n",
    "plt.plot(baseNetwork_metrics_df['mcc'], label=\"Basic Neural Network\")\n",
    "plt.plot(smallNetwork_metrics_df['mcc'], label=\"Small Nueral Network\")\n",
    "plt.plot(denseNetwork_metrics_df['mcc'], label=\"Big Neural Network\")\n",
    "plt.plot(cnn_metrics_df['mcc'], label=\"Convoluted Neural Network\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MCC score\")\n",
    "plt.title(\"MCC scores over Iterations\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(baseNetwork_metrics_df['f1'], label=\"Basic Neural Network\")\n",
    "plt.plot(smallNetwork_metrics_df['f1'], label=\"Small Nueral Network\")\n",
    "plt.plot(denseNetwork_metrics_df['f1'], label=\"Big Neural Network\")\n",
    "plt.plot(cnn_metrics_df['f1'], label=\"Convoluted Neural Network\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.title(\"F1 scores Over Iterations\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to file\n",
    "# index=False to prevent extra index column\n",
    "cnn_metrics_df.to_csv('cnn_metrics.csv', index=False)\n",
    "denseNetwork_metrics_df.to_csv('denseNetwork_metrics.csv', index=False)\n",
    "smallNetwork_metrics_df.to_csv('smallNetwork_metrics.csv', index=False)\n",
    "baseNetwork_metrics_df.to_csv('baseNetwork_metrics.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"CSV file saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scripts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
